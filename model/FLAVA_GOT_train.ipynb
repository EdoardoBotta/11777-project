{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY80UV6KtGa_",
        "outputId": "8a8c789c-4b7b-4cb1-ee92-097153d90d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.5\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip3 install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6wG3GSb-Pfe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF6_jYNy9GDb",
        "outputId": "1a0a2be8-75de-4d8b-b78a-5fa419c788bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "token = \"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\"\n",
        "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
        "winoground = load_dataset(\"facebook/winoground\", use_auth_token=token)[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9G5u2Uq39EZ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "from tqdm import tqdm\n",
        "\n",
        "# utility function to plot examples\n",
        "def plot_example(i, winoground):\n",
        "    max_width = max([winoground[i][f\"image_{j}\"].width for j in (0,1)])\n",
        "    max_height = max([winoground[i][f\"image_{j}\"].height for j in (0,1)])\n",
        "\n",
        "    canvas_width = max_width*2\n",
        "    canvas_height = max_height\n",
        "\n",
        "    canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))\n",
        "\n",
        "    (img0, img1) = winoground[i][\"image_0\"], winoground[i][\"image_1\"]\n",
        "\n",
        "    print(f\"Left caption: {winoground[i]['caption_0']}\")\n",
        "    print(f\"Right caption: {winoground[i]['caption_1']}\")\n",
        "\n",
        "    canvas.paste(img0, (0, 0))\n",
        "    canvas.paste(img1, (canvas_width//2, 0))\n",
        "\n",
        "    return canvas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FlavaProcessor, FlavaForPreTraining, FlavaModel\n",
        "\n",
        "model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\").to(device)\n",
        "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7TVlHO5unyd",
        "outputId": "e1f9dfb6-70be-4426-9d1f-de996c038e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
            "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "winoground_flava_contrastive_scores = []\n",
        "winoground_flava_itm_scores = []\n",
        "for example in tqdm(winoground):\n",
        "  # Note that some images in winoground are RGBA and some are RGB. Need to convert all to RGB with .convert('RGB')\n",
        "  inputs_c0_i0 = processor(text=[example[\"caption_0\"]], images=[example[\"image_0\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n",
        "  inputs_c1_i0 = processor(text=[example[\"caption_1\"]], images=[example[\"image_0\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n",
        "  inputs_c0_i1 = processor(text=[example[\"caption_0\"]], images=[example[\"image_1\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n",
        "  inputs_c1_i1 = processor(text=[example[\"caption_1\"]], images=[example[\"image_1\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n",
        "\n",
        "  inputs_c0_i0[\"input_ids_masked\"] = inputs_c0_i0[\"input_ids\"].detach().clone()\n",
        "  inputs_c1_i0[\"input_ids_masked\"] = inputs_c1_i0[\"input_ids\"].detach().clone()\n",
        "  inputs_c0_i1[\"input_ids_masked\"] = inputs_c0_i1[\"input_ids\"].detach().clone()\n",
        "  inputs_c1_i1[\"input_ids_masked\"] = inputs_c1_i1[\"input_ids\"].detach().clone()\n",
        "\n",
        "  inputs_c0_i0[\"bool_masked_pos\"] = torch.zeros_like(inputs_c0_i0[\"bool_masked_pos\"])\n",
        "  inputs_c1_i0[\"bool_masked_pos\"] = torch.zeros_like(inputs_c1_i0[\"bool_masked_pos\"])\n",
        "  inputs_c0_i1[\"bool_masked_pos\"] = torch.zeros_like(inputs_c0_i1[\"bool_masked_pos\"])\n",
        "  inputs_c1_i1[\"bool_masked_pos\"] = torch.zeros_like(inputs_c1_i1[\"bool_masked_pos\"])\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs_c0_i0 = model(**inputs_c0_i0)\n",
        "    outputs_c1_i0 = model(**inputs_c1_i0)\n",
        "    outputs_c0_i1 = model(**inputs_c0_i1)\n",
        "    outputs_c1_i1 = model(**inputs_c1_i1)\n",
        "\n",
        "  flava_contrastive_scores_c0_i0 = outputs_c0_i0.contrastive_logits_per_image.item()\n",
        "  flava_contrastive_scores_c1_i0 = outputs_c1_i0.contrastive_logits_per_image.item()\n",
        "  flava_contrastive_scores_c0_i1 = outputs_c0_i1.contrastive_logits_per_image.item()\n",
        "  flava_contrastive_scores_c1_i1 = outputs_c1_i1.contrastive_logits_per_image.item()\n",
        "  winoground_flava_contrastive_scores.append({\"id\" : example[\"id\"], \"c0_i0\": flava_contrastive_scores_c0_i0, \"c0_i1\": flava_contrastive_scores_c0_i1, \"c1_i0\": flava_contrastive_scores_c1_i0, \"c1_i1\": flava_contrastive_scores_c1_i1})\n",
        "\n",
        "  flava_itm_scores_c0_i0 = torch.nn.functional.softmax(outputs_c0_i0.itm_logits)[0][1].item()\n",
        "  flava_itm_scores_c1_i0 = torch.nn.functional.softmax(outputs_c1_i0.itm_logits)[0][1].item()\n",
        "  flava_itm_scores_c0_i1 = torch.nn.functional.softmax(outputs_c0_i1.itm_logits)[0][1].item()\n",
        "  flava_itm_scores_c1_i1 = torch.nn.functional.softmax(outputs_c1_i1.itm_logits)[0][1].item()\n",
        "  winoground_flava_itm_scores.append({\"id\" : example[\"id\"], \"c0_i0\": flava_itm_scores_c0_i0, \"c0_i1\": flava_itm_scores_c0_i1, \"c1_i0\": flava_itm_scores_c1_i0, \"c1_i1\": flava_itm_scores_c1_i1})\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QkCSqwRNgkv",
        "outputId": "b32576fe-dd66-40bb-a9d4-3e729324eb8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/400 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-a8425cd07bd5>:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  flava_itm_scores_c0_i0 = torch.nn.functional.softmax(outputs_c0_i0.itm_logits)[0][1].item()\n",
            "<ipython-input-12-a8425cd07bd5>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  flava_itm_scores_c1_i0 = torch.nn.functional.softmax(outputs_c1_i0.itm_logits)[0][1].item()\n",
            "<ipython-input-12-a8425cd07bd5>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  flava_itm_scores_c0_i1 = torch.nn.functional.softmax(outputs_c0_i1.itm_logits)[0][1].item()\n",
            "<ipython-input-12-a8425cd07bd5>:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  flava_itm_scores_c1_i1 = torch.nn.functional.softmax(outputs_c1_i1.itm_logits)[0][1].item()\n",
            "  0%|          | 0/400 [00:06<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_embeddings, text_embeddings = outputs_c0_i0.image_embeddings, outputs_c0_i0.text_embeddings"
      ],
      "metadata": {
        "id": "ppmX8flHO666"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FlavaProcessor, FlavaForPreTraining, FlavaModel\n",
        "\n",
        "import collections\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
        "from transformers.modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n",
        "from transformers.utils import (\n",
        "    ModelOutput,\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers.models.flava.configuration_flava import (\n",
        "    FlavaConfig,\n",
        "    FlavaImageCodebookConfig,\n",
        "    FlavaImageConfig,\n",
        "    FlavaMultimodalConfig,\n",
        "    FlavaTextConfig,\n",
        ")\n",
        "from transformers.models.flava.modeling_flava import (\n",
        "    FlavaForPreTrainingOutput,\n",
        "    FlavaLosses,\n",
        "    FLAVA_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
        "    FLAVA_CODEBOOK_PRETRAINED_MODEL_ARCHIVE_LIST,\n",
        "    LOGIT_SCALE_CLAMP_MIN,\n",
        "    LOGIT_SCALE_CLAMP_MAX,\n",
        "    FlavaPossibleConfigs,\n",
        "    logger\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class FlavaGOTLosses(FlavaLosses):\n",
        "    got: Optional[torch.FloatTensor] = None\n",
        "\n",
        "\n",
        "class FlavaGOTConfig(FlavaConfig):\n",
        "    def __init__(self, got_weight=1.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.got_weight = got_weight\n",
        "\n",
        "@dataclass\n",
        "class FlavaGOTForPreTrainingOutput(FlavaForPreTrainingOutput):\n",
        "    loss_info: Optional[FlavaGOTLosses] = None\n",
        "\n",
        "\n",
        "class FlavaGOTForPreTraining(FlavaForPreTraining):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.got_loss = GOTLoss\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        input_ids_masked: Optional[torch.LongTensor] = None,\n",
        "        pixel_values: Optional[torch.FloatTensor] = None,\n",
        "        codebook_pixel_values: Optional[torch.FloatTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        bool_masked_pos: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        image_attention_mask: Optional[torch.Tensor] = None,\n",
        "        skip_unmasked_multimodal_encoder: bool = None,\n",
        "        mlm_labels: Optional[torch.Tensor] = None,\n",
        "        mim_labels: Optional[torch.Tensor] = None,\n",
        "        itm_labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: bool = True,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        return_loss: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], FlavaGOTForPreTrainingOutput]:\n",
        "        \"\"\"\n",
        "        Examples:\n",
        "        ```python\n",
        "        >>> from PIL import Image\n",
        "        >>> import requests\n",
        "        >>> from transformers import FlavaForPreTraining, AutoProcessor\n",
        "\n",
        "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "        >>> model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\")\n",
        "        >>> processor = AutoProcessor.from_pretrained(\"facebook/flava-full\")\n",
        "\n",
        "        >>> text = [\"a photo of a cat\"]\n",
        "\n",
        "        >>> inputs = processor(\n",
        "        ...     images=[image],\n",
        "        ...     text=text,\n",
        "        ...     return_masks=True,\n",
        "        ...     return_codebook_pixels=True,\n",
        "        ...     padding=True,\n",
        "        ...     max_length=77,\n",
        "        ...     return_tensors=\"pt\",\n",
        "        ... )\n",
        "\n",
        "\n",
        "        >>> output = model(**inputs)\n",
        "        ```\n",
        "\n",
        "        Return:\n",
        "\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        return_loss = return_loss if return_loss is not None else self.config.return_loss\n",
        "\n",
        "        skip_unmasked_multimodal_encoder = (\n",
        "            skip_unmasked_multimodal_encoder\n",
        "            if skip_unmasked_multimodal_encoder is not None\n",
        "            else self.skip_unmasked_multimodal_encoder\n",
        "        )\n",
        "\n",
        "        if input_ids_masked is None and input_ids is not None:\n",
        "            logger.warning(\n",
        "                \"`input_ids_masked` isn't passed which means MLM loss won't be calculated correctlySetting it to\"\n",
        "                \" `input_ids` so that model can work. Please pass it if this is unintentional. This is usually OKAY if\"\n",
        "                \" you are doing inference on unmasked text...\"\n",
        "            )\n",
        "            input_ids_masked = input_ids\n",
        "\n",
        "        flava_output = self.flava(\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            image_attention_mask=image_attention_mask,\n",
        "            # Don't need unmasked multimodal embedding for anything so skip it\n",
        "            # NOTE: ITM uses masked version\n",
        "            skip_multimodal_encoder=skip_unmasked_multimodal_encoder,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            # Pass true to have deterministic outputs\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        flava_masked_output = self.flava(\n",
        "            input_ids=input_ids_masked,\n",
        "            pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            image_attention_mask=image_attention_mask,\n",
        "            bool_masked_pos=bool_masked_pos,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        pos_mask = None\n",
        "\n",
        "        image_embeddings = flava_output.image_embeddings\n",
        "        text_embeddings = flava_output.text_embeddings\n",
        "        image_masked_embeddings = flava_masked_output.image_embeddings\n",
        "        text_masked_embeddings = flava_masked_output.text_embeddings\n",
        "        multimodal_masked_embeddings = flava_masked_output.multimodal_embeddings\n",
        "\n",
        "        total_loss = mim_loss = mlm_loss = mmm_text_loss = mmm_image_loss = gc_loss = itm_loss = None\n",
        "        mim_logits = mlm_logits = mmm_text_logits = mmm_image_logits = None\n",
        "        itm_logits = logits_per_image = logits_per_text = None\n",
        "\n",
        "        # Calculate mim_labels if necessary from the image_codebook\n",
        "        if image_masked_embeddings is not None or multimodal_masked_embeddings is not None:\n",
        "            if mim_labels is None and return_loss:\n",
        "                if self.image_codebook is None:\n",
        "                    raise RuntimeError(\n",
        "                        \"`return_loss` is set to True but the image codebook is not initialized and no `mim_labels` \"\n",
        "                        \" have been passed. Reinstantiate the model with `init_codebook` set to True or \"\n",
        "                        \"pass in your custom `mim_labels`\"\n",
        "                    )\n",
        "                if codebook_pixel_values is None:\n",
        "                    raise ValueError(\n",
        "                        \"`codebook_pixel_value` are required to generate `mim_labels` if loss is expected. \"\n",
        "                        \"Call `AutoProcessor` with `return_codebook_pixels` set to True\"\n",
        "                    )\n",
        "                mim_labels = self.image_codebook.get_codebook_indices(codebook_pixel_values)\n",
        "        # Unimodal MIM Loss\n",
        "        # If multimodal embeddings are present, we will calculate MMM loss\n",
        "        if self.mim_weight > 0 and image_masked_embeddings is not None and multimodal_masked_embeddings is None:\n",
        "            sequence_for_image = image_masked_embeddings\n",
        "\n",
        "            if mim_labels is not None:\n",
        "                mim_labels = self._resize_to_2d(mim_labels)\n",
        "                bool_masked_pos = self._resize_to_2d(bool_masked_pos)\n",
        "                mim_labels[bool_masked_pos.ne(True)] = self.ce_ignore_index\n",
        "\n",
        "                sequence_for_image = sequence_for_image[:, -mim_labels.size(1) :, :]\n",
        "                masked_tokens = mim_labels.ne(self.ce_ignore_index)\n",
        "                mim_labels_filtered = mim_labels[masked_tokens]\n",
        "                sequence_for_image = sequence_for_image[masked_tokens, :]\n",
        "                mim_logits = self.mim_head(sequence_for_image)\n",
        "                if return_loss:\n",
        "                    mim_loss = nn.functional.cross_entropy(\n",
        "                        mim_logits.view(-1, self.image_vocab_size), mim_labels_filtered.view(-1)\n",
        "                    )\n",
        "                    mim_loss *= self.mim_weight\n",
        "            else:\n",
        "                mim_logits = self.mim_head(sequence_for_image)\n",
        "\n",
        "        # Unimodal MLM Loss\n",
        "        if self.mlm_weight > 0 and text_masked_embeddings is not None and multimodal_masked_embeddings is None:\n",
        "            sequence_for_text = text_masked_embeddings\n",
        "            if mlm_labels is not None:\n",
        "                mlm_labels = self._resize_to_2d(mlm_labels)\n",
        "                sequence_for_text = sequence_for_text[:, -mlm_labels.size(1) :, :]\n",
        "                masked_tokens = mlm_labels.ne(self.ce_ignore_index)\n",
        "                mlm_labels_filtered = mlm_labels[masked_tokens]\n",
        "                sequence_for_text = sequence_for_text[masked_tokens, :]\n",
        "                mlm_logits = self.mlm_head(sequence_for_text)\n",
        "                if return_loss:\n",
        "                    mlm_loss = nn.functional.cross_entropy(\n",
        "                        mlm_logits.view(-1, self.text_vocab_size), mlm_labels_filtered.view(-1)\n",
        "                    )\n",
        "                    mlm_loss *= self.mlm_weight\n",
        "            else:\n",
        "                mlm_logits = self.mlm_head(sequence_for_text)\n",
        "\n",
        "        # ITM Loss\n",
        "        if self.itm_weight > 0 and multimodal_masked_embeddings is not None:\n",
        "            itm_logits = self.itm_head(multimodal_masked_embeddings)\n",
        "\n",
        "            if itm_labels is not None:\n",
        "                pos_pairs = itm_labels.ne(0)\n",
        "                pos_mask = torch.where(pos_pairs.any(), pos_pairs, pos_pairs.new([True]))\n",
        "                if return_loss:\n",
        "                    itm_loss = nn.functional.cross_entropy(itm_logits, itm_labels)\n",
        "                    itm_loss *= self.itm_weight\n",
        "\n",
        "                if multimodal_masked_embeddings is not None:\n",
        "                    multimodal_masked_embeddings = multimodal_masked_embeddings[pos_mask]\n",
        "\n",
        "                if mlm_labels is not None:\n",
        "                    mlm_labels = mlm_labels[pos_mask]\n",
        "\n",
        "                if mim_labels is not None:\n",
        "                    mim_labels = mim_labels[pos_mask]\n",
        "\n",
        "        # GOT Regularization Loss (TODO: Integrate GOT code here)\n",
        "        got_loss = self.got_loss(image_embeddings, text_embeddings)\n",
        "\n",
        "        # MMM Image Loss\n",
        "        if multimodal_masked_embeddings is not None and self.mmm_image_weight > 0:\n",
        "            sequence_for_image = multimodal_masked_embeddings\n",
        "            end_index = image_masked_embeddings.size(1) - 1\n",
        "            sequence_for_image = sequence_for_image[:, 2 : 2 + end_index, :]\n",
        "\n",
        "            if pos_mask is not None:\n",
        "                sequence_for_image = sequence_for_image[pos_mask]\n",
        "            if mim_labels is not None:\n",
        "                mim_labels = self._resize_to_2d(mim_labels)\n",
        "                bool_masked_pos = self._resize_to_2d(bool_masked_pos)\n",
        "                mim_labels[bool_masked_pos.ne(True)] = self.ce_ignore_index\n",
        "\n",
        "                masked_tokens = mim_labels.ne(self.ce_ignore_index)\n",
        "                mim_labels_filtered = mim_labels[masked_tokens]\n",
        "                sequence_for_image = sequence_for_image[masked_tokens, :]\n",
        "                mmm_image_logits = self.mmm_image_head(sequence_for_image)\n",
        "                if return_loss:\n",
        "                    mmm_image_loss = nn.functional.cross_entropy(\n",
        "                        mmm_image_logits.view(-1, self.image_vocab_size), mim_labels_filtered.view(-1)\n",
        "                    )\n",
        "                    mmm_image_loss *= self.mmm_image_weight\n",
        "            else:\n",
        "                mmm_image_logits = self.mmm_image_head(sequence_for_image)\n",
        "\n",
        "        # MMM Text Loss\n",
        "        if multimodal_masked_embeddings is not None and self.mmm_text_weight > 0:\n",
        "            sequence_for_text = multimodal_masked_embeddings\n",
        "            sequence_for_text = sequence_for_text[:, -text_masked_embeddings.size(1) :, :]\n",
        "            if pos_mask is not None:\n",
        "                sequence_for_text = sequence_for_text[pos_mask]\n",
        "\n",
        "            if mlm_labels is not None:\n",
        "                mlm_labels = self._resize_to_2d(mlm_labels)\n",
        "                masked_tokens = mlm_labels.ne(self.ce_ignore_index)\n",
        "                mlm_labels_filtered = mlm_labels[masked_tokens]\n",
        "                sequence_for_text = sequence_for_text[masked_tokens, :]\n",
        "                mmm_text_logits = self.mmm_text_head(sequence_for_text)\n",
        "                if return_loss:\n",
        "                    mmm_text_loss = nn.functional.cross_entropy(\n",
        "                        mmm_text_logits.view(-1, self.text_vocab_size), mlm_labels_filtered.view(-1)\n",
        "                    )\n",
        "                    mmm_text_loss *= self.mmm_text_weight\n",
        "            else:\n",
        "                mmm_text_logits = self.mmm_text_head(sequence_for_text)\n",
        "\n",
        "        # Global Contrastive Loss\n",
        "        if image_embeddings is not None and text_embeddings is not None and self.global_contrastive_weight > 0:\n",
        "            text_embedding = self.flava.text_projection(text_embeddings[:, 0, :])\n",
        "            text_embedding = nn.functional.normalize(text_embedding, dim=-1)\n",
        "\n",
        "            image_embedding = self.flava.image_projection(image_embeddings[:, 0, :])\n",
        "            image_embedding = nn.functional.normalize(image_embedding, dim=-1)\n",
        "\n",
        "            self.flava.logit_scale.data.clamp_(LOGIT_SCALE_CLAMP_MIN, LOGIT_SCALE_CLAMP_MAX)\n",
        "\n",
        "            logits_per_image, logits_per_text, gc_labels = self.global_contrastive_head(\n",
        "                image_embedding, text_embedding, self.flava.logit_scale\n",
        "            )\n",
        "\n",
        "            # Apply ITM negative mask if any\n",
        "            if pos_mask is not None:\n",
        "                logits_per_image = logits_per_image[pos_mask]\n",
        "                logits_per_text = logits_per_text[pos_mask]\n",
        "                gc_labels = gc_labels[pos_mask]\n",
        "\n",
        "            if return_loss:\n",
        "                gc_loss_image = nn.functional.cross_entropy(logits_per_image, gc_labels)\n",
        "                gc_loss_text = nn.functional.cross_entropy(logits_per_text, gc_labels)\n",
        "                gc_loss = (gc_loss_image + gc_loss_text) / 2\n",
        "                gc_loss *= self.global_contrastive_weight\n",
        "\n",
        "        flava_losses = FlavaGOTLosses(\n",
        "            got=got_loss,\n",
        "            mim=mim_loss,\n",
        "            mlm=mlm_loss,\n",
        "            itm=itm_loss,\n",
        "            global_contrastive=gc_loss,\n",
        "            mmm_image=mmm_image_loss,\n",
        "            mmm_text=mmm_text_loss,\n",
        "        )\n",
        "\n",
        "        if return_loss and not flava_losses.all_none():\n",
        "            total_loss = sum(loss if loss is not None else 0 for loss in flava_losses.values())\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (\n",
        "                image_embeddings,\n",
        "                flava_output.image_output.to_tuple() if flava_output.image_output is not None else None,\n",
        "                text_embeddings,\n",
        "                flava_output.text_output.to_tuple() if flava_output.text_output is not None else None,\n",
        "                flava_output.multimodal_embeddings,\n",
        "                flava_output.multimodal_output.to_tuple() if flava_output.multimodal_output is not None else None,\n",
        "                image_masked_embeddings,\n",
        "                flava_masked_output.image_output.to_tuple() if flava_masked_output.image_output is not None else None,\n",
        "                text_masked_embeddings,\n",
        "                flava_masked_output.text_output.to_tuple() if flava_masked_output.text_output is not None else None,\n",
        "                multimodal_masked_embeddings,\n",
        "                flava_masked_output.multimodal_output.to_tuple()\n",
        "                if flava_masked_output.multimodal_output is not None\n",
        "                else None,\n",
        "                mim_logits,\n",
        "                mlm_logits,\n",
        "                itm_logits,\n",
        "                logits_per_image,\n",
        "                logits_per_image,\n",
        "                mmm_image_logits,\n",
        "                mmm_text_logits,\n",
        "            )\n",
        "            if return_loss and not flava_losses.all_none():\n",
        "                output = (\n",
        "                    total_loss,\n",
        "                    flava_losses,\n",
        "                ) + output\n",
        "\n",
        "            # Filter None as transformer by default won't handle it\n",
        "            return tuple(x for x in output if x is None)\n",
        "\n",
        "        return FlavaGOTForPreTrainingOutput(\n",
        "            loss=total_loss,\n",
        "            loss_info=flava_losses,\n",
        "            image_embeddings=image_embeddings,\n",
        "            image_output=flava_output.image_output,\n",
        "            text_embeddings=text_embeddings,\n",
        "            text_output=flava_output.text_output,\n",
        "            multimodal_embeddings=flava_output.multimodal_embeddings,\n",
        "            multimodal_output=flava_output.multimodal_output,\n",
        "            image_masked_embeddings=image_masked_embeddings,\n",
        "            image_masked_output=flava_masked_output.image_output,\n",
        "            text_masked_embeddings=text_masked_embeddings,\n",
        "            text_masked_output=flava_masked_output.text_output,\n",
        "            multimodal_masked_embeddings=multimodal_masked_embeddings,\n",
        "            multimodal_masked_output=flava_masked_output.multimodal_output,\n",
        "            mim_logits=mim_logits,\n",
        "            mlm_logits=mlm_logits,\n",
        "            itm_logits=itm_logits,\n",
        "            contrastive_logits_per_image=logits_per_image,\n",
        "            contrastive_logits_per_text=logits_per_text,\n",
        "            mmm_image_logits=mmm_image_logits,\n",
        "            mmm_text_logits=mmm_text_logits,\n",
        "        )"
      ],
      "metadata": {
        "id": "dUsxcW6xxzbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCwfGephXcib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal Transport"
      ],
      "metadata": {
        "id": "29bvidbG3DAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from functools import partial\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from torch.autograd import Variable\n",
        "import pdb\n",
        "\n",
        "def cost_matrix_torch(x, y):\n",
        "\t\"Returns the cosine distance\"\n",
        "\t# x is the image embedding\n",
        "\t# y is the text embedding\n",
        "\tD = x.size(0)\n",
        "\tx = x.view(D, -1)\n",
        "\tassert(x.size(0)==y.size(0))\n",
        "\tx = x.div(torch.norm(x, p=2, dim=0, keepdim=True) + 1e-12)\n",
        "\ty = y.div(torch.norm(y, p=2, dim=0, keepdim=True) + 1e-12)\n",
        "\tcos_dis = torch.mm(torch.transpose(y,0,1), x)#.t()\n",
        "\tcos_dis = 1 - cos_dis # to minimize this value\n",
        "\treturn cos_dis\n",
        "\n",
        "def IPOT_torch(C, n, m, miu, nu, beta=0.5):\n",
        "\t# C is the distance matrix\n",
        "\t# c: n by m\n",
        "\t# miu: bs * n\n",
        "\tsigma = torch.ones(int(m), 1).float().cuda()/m # bs * m * 1\n",
        "\tT = torch.ones(n, m).cuda()\n",
        "\tC = torch.exp(-C/beta).float()\n",
        "\tfor t in range(20):\n",
        "\t\tT = C * T # n * m\n",
        "\t\tfor k in range(1):\n",
        "\t\t\tdelta = miu / torch.squeeze(torch.matmul(T, sigma))\n",
        "\t\t\t# a = torch.matmul(torch.transpose(T,0,1), torch.unsqueeze(delta,1))\n",
        "\t\t\t# sigma = torch.unsqueeze(nu,1) / a\n",
        "\t\t\tsigma = torch.unsqueeze(nu,1) / torch.matmul(torch.transpose(T,0,1), torch.unsqueeze(delta,1))\n",
        "\t\t# tmp = torch.mm(torch.diag(torch.squeeze(delta)), Q)\n",
        "\t\t# tmp = torch.unsqueeze(delta,1) * A\n",
        "\t\t# dim_ = torch.diag(torch.squeeze(sigma)).dim()\n",
        "\t\t# dim_ = torch.diag(torch.squeeze(sigma)).dim()\n",
        "\t\t# assert (dim_ == 2 or dim_ == 1, \"dim_ is %d\" % dim_)\n",
        "\t\t# T = torch.mm(torch.unsqueeze(delta,1) * T, torch.diag(torch.squeeze(sigma)))\n",
        "\t\tT = torch.unsqueeze(delta,1) * T * sigma.transpose(1,0)\n",
        "\treturn T.detach()\n",
        "\n",
        "def IPOT_distance_torch(C, n, m, miu, nu):\n",
        "\tC = C.float().cuda()\n",
        "\tT = IPOT_torch(C, n, m, miu, nu)\n",
        "\tdistance = torch.trace(torch.mm(torch.transpose(C,0,1), T))\n",
        "\treturn -distance\n",
        "\n",
        "\n",
        "def IPOT_distance_torch_batch(C, n, m, miu, nu, iteration):\n",
        "\t# C as a 2 d matrix\n",
        "\tC = C.float().cuda()\n",
        "\tbs = miu.size(0)\n",
        "\t# if C.dim()==2:\n",
        "\t# \tC=C.repeat(bs, 1, 1)\n",
        "\tif C.dim()==2:\n",
        "\t\tC = torch.unsqueeze(C, 0)\n",
        "\t# if not bs == C.size(0):\n",
        "\t# \tprint('break')\n",
        "\t# assert(bs == C.size(0))\n",
        "\tT = IPOT_torch_batch(C, bs, n, m, miu, nu, iteration)\n",
        "\ttemp = torch.matmul(torch.transpose(C,1,2), T)\n",
        "\tdistance = batch_trace(temp, m, bs)\n",
        "\treturn -distance\n",
        "\n",
        "\n",
        "def IPOT_torch_batch(C, bs, n, m, miu, nu, iteration=20, beta=0.5):\n",
        "\t# C is the distance matrix, 2d matrix\n",
        "\t# c: n by m\n",
        "\t# miu: bs * n\n",
        "\tsigma = torch.ones(bs, int(m), 1).cuda().detach()/float(m) # bs * m * 1\n",
        "\tQ = torch.ones(bs, n, m).cuda().detach().float()\n",
        "\tC = torch.exp(-C/beta)#.unsqueeze(0)\n",
        "\tif nu.dim() < 3:\n",
        "\t\tnu = torch.unsqueeze(nu,2)\n",
        "\t# if miu.dim()<3:\n",
        "\t# \tmiu = torch.unsqueeze(miu,1)\n",
        "\tmiu = torch.squeeze(miu)\n",
        "\tfor t in range(iteration):\n",
        "\t\tQ = C * Q # bs * n * m\n",
        "\t\tfor k in range(1):\n",
        "\t\t\tdelta = torch.unsqueeze((miu / torch.squeeze(torch.bmm(Q, sigma)+1e-6)),2)\n",
        "\t\t\t# delta = ((miu / (torch.bmm(Q, sigma) + 1e-6)))\n",
        "\t\t\ta = torch.bmm(torch.transpose(Q,1,2), delta)+1e-6\n",
        "\t\t\tsigma = nu / a\n",
        "\t\tQ = delta * Q * sigma.transpose(2,1)\n",
        "\t\t# Q = torch.matmul(tmp, diag_sigma)\n",
        "\treturn Q.detach()\n",
        "\n",
        "def IPOT_torch_uniform(C, n, m, beta=0.5):\n",
        "\t# C is the distance matrix\n",
        "\tsigma = torch.ones(int(m), 1).cuda()/m\n",
        "\tT = torch.ones(n, m).cuda()\n",
        "\tA = torch.exp(-C/beta)\n",
        "\tfor t in range(50):\n",
        "\t\tQ = A * T # n * m\n",
        "\t\tfor k in range(1):\n",
        "\t\t\tdelta = 1 / (n * torch.mm(Q, sigma))\n",
        "\t\t\ta = torch.mm(torch.transpose(Q,0,1), delta)\n",
        "\t\t\tsigma = 1 / (float(m) * a)\n",
        "\t\ttmp = torch.mm(torch.diag(torch.squeeze(delta)), Q)\n",
        "\t\tdim_ = torch.diag(torch.squeeze(sigma)).dim()\n",
        "\t\tassert (dim_ == 2 or dim_ == 1)\n",
        "\t\tT = torch.mm(tmp, torch.diag(torch.squeeze(sigma)))\n",
        "\treturn T.detach()\n",
        "\n",
        "def IPOT_distance_torch_uniform(C, n, m):\n",
        "\tC = C.float().cuda()\n",
        "\tT = IPOT_torch_uniform(C, n, m)\n",
        "\tdistance = torch.trace(torch.mm(torch.transpose(C,0,1), T))\n",
        "\treturn distance\n",
        "\n",
        "\n",
        "def cost_matrix_batch_torch(x, y):\n",
        "\t\"Returns the cosine distance batchwise\"\n",
        "\t# x is the image feature: bs * d * m * m\n",
        "\t# y is the audio feature: bs * d * nF\n",
        "\t# return: bs * n * m\n",
        "\t# print(x.size())\n",
        "\tbs = list(x.size())[0]\n",
        "\tD = x.size(1)\n",
        "\tassert(x.size(1)==y.size(1))\n",
        "\tx = x.contiguous().view(bs, D, -1) # bs * d * m^2\n",
        "\tx = x.div(torch.norm(x, p=2, dim=1, keepdim=True) + 1e-12)\n",
        "\ty = y.div(torch.norm(y, p=2, dim=1, keepdim=True) + 1e-12)\n",
        "\tcos_dis = torch.bmm(torch.transpose(x, 1, 2), y)#.transpose(1,2)\n",
        "\tcos_dis = 1 - cos_dis # to minimize this value\n",
        "\t# cos_dis = - cos_dis\n",
        "\treturn cos_dis.transpose(2,1)\n",
        "\n",
        "\n",
        "def cost_matrix_batch_torch_acos(x, y):\n",
        "\t\"Returns the cosine distance batchwise\"\n",
        "\t# x is the image feature: bs * d * m * m\n",
        "\t# y is the audio feature: bs * d * nF\n",
        "\t# return: bs * n * m\n",
        "\t# print(x.size())\n",
        "\tbs = list(x.size())[0]\n",
        "\tD = x.size(1)\n",
        "\tassert(x.size(1)==y.size(1))\n",
        "\tx = x.contiguous().view(bs, D, -1) # bs * d * m^2\n",
        "\tx = x.div(torch.norm(x, p=2, dim=1, keepdim=True) + 1e-12)\n",
        "\ty = y.div(torch.norm(y, p=2, dim=1, keepdim=True) + 1e-12)\n",
        "\tcos_dis = torch.bmm(torch.transpose(x,1,2), y)#.transpose(1,2)\n",
        "\tcos_dis = torch.acos(cos_dis) # to minimize this value\n",
        "\t# cos_dis = - cos_dis\n",
        "\treturn cos_dis.transpose(2,1)\n",
        "\n",
        "def cos_batch_torch(x, y):\n",
        "\t\"Returns the cosine distance batchwise\"\n",
        "\t# x is the image feature: bs * d * m * m\n",
        "\t# y is the audio feature: bs * d * nF\n",
        "\t# return: bs * n * m\n",
        "\t# print(x.size())\n",
        "\tbs = x.size(0)\n",
        "\tD = x.size(1)\n",
        "\tassert(x.size(1)==y.size(1))\n",
        "\tx = x.contiguous().view(bs, D, -1) # bs * d * m^2\n",
        "\tx = x.div(torch.norm(x, p=2, dim=1, keepdim=True) + 1e-12)\n",
        "\ty = y.div(torch.norm(y, p=2, dim=1, keepdim=True) + 1e-12)\n",
        "\tcos_dis = torch.bmm(torch.transpose(x,1,2), y)#.transpose(1,2)\n",
        "\tcos_dis = 1 - cos_dis # to minimize this value\n",
        "\t# return cos_dis.transpose(2,1)\n",
        "\t# TODO:\n",
        "\tbeta = 0.1\n",
        "\tmin_score = cos_dis.min()\n",
        "\tmax_score = cos_dis.max()\n",
        "\tthreshold = min_score + beta * (max_score - min_score)\n",
        "\tres = cos_dis - threshold\n",
        "\t# res = torch.nn.ReLU()\n",
        "\n",
        "\treturn torch.nn.functional.relu(res.transpose(2,1))\n",
        "\n",
        "\n",
        "def pairwise_distances(x, y=None):\n",
        "\t'''\n",
        "\tInput: x is a Nxd matrix\n",
        "\t\t   y is an optional Mxd matirx\n",
        "\tOutput: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
        "\t\t\tif y is not given then use 'y=x'.\n",
        "\ti.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
        "\t'''\n",
        "\tx_norm = (x ** 2).sum(1).view(-1, 1)\n",
        "\tif y is not None:\n",
        "\t\ty_t = torch.transpose(y, 0, 1)\n",
        "\t\ty_norm = (y ** 2).sum(1).view(1, -1)\n",
        "\telse:\n",
        "\t\ty_t = torch.transpose(x, 0, 1)\n",
        "\t\ty_norm = x_norm.view(1, -1)\n",
        "\n",
        "\tdist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
        "\t# Ensure diagonal is zero if x=y\n",
        "\t# if y is None:\n",
        "\t#     dist = dist - torch.diag(dist.diag)\n",
        "\treturn torch.clamp(dist, 0.0, np.inf)\n",
        "\n",
        "def row_pairwise_distances(x, y=None, dist_mat=None):\n",
        "    if y is None:\n",
        "        y = x\n",
        "    if dist_mat is None:\n",
        "        dtype = x.data.type()\n",
        "        dist_mat = Variable(torch.Tensor(x.size()[0], y.size()[0]).type(dtype))\n",
        "\n",
        "    for i, row in enumerate(x.split(1)):\n",
        "        r_v = row.expand_as(y)\n",
        "        sq_dist = torch.sum((r_v - y) ** 2, 1)\n",
        "        dist_mat[i] = sq_dist.view(1, -1)\n",
        "    return dist_mat\n",
        "\n",
        "def IPOT_barycenter(p, C, q, iteration=20, beta=0.5, iteration_inner = 1):\n",
        "\t'''\n",
        "\n",
        "\t:param p: probability vector set, K x n\n",
        "\t:param C: cost matrix, K x n x n\n",
        "\t:param q: initial q, mean of all support, n x d\n",
        "\t:return:\n",
        "\t'''\n",
        "\tK = p.size(0)\n",
        "\tn = p.size(1)\n",
        "\tassert(C.size(1)==C.size(2))\n",
        "\tassert(C.size(1)==p.size(1))\n",
        "\tb = torch.ones(K, int(n), 1).cuda().detach()/float(n) # bs * m * 1\n",
        "\tC = torch.exp(-C/beta)\n",
        "\tT = torch.ones(K, n, n).cuda().detach().float()\n",
        "\tq = torch.unsqueeze(q, 0)\n",
        "\tfor t in range(iteration):\n",
        "\t\tH = T * C\n",
        "\t\tfor k in range(iteration_inner):\n",
        "\t\t\ta = q/torch.bmm(H, b)\n",
        "\t\t\tb = p/torch.bmm(torch.transpose(H, 2, 1), a)\n",
        "\t\t\tq = a * (torch.bmm(H, b))\n",
        "\t\tT = a * H * b.transpose(2,1)\n",
        "\treturn q\n",
        "\n",
        "\n",
        "def IPOT_distance_torch_batch_uniform(C, bs, n, m, iteration=50):\n",
        "\tC = C.float().cuda()\n",
        "\tT = IPOT_torch_batch_uniform(C, bs, n, m, iteration=iteration)\n",
        "\ttemp = torch.bmm(torch.transpose(C,1,2), T)\n",
        "\tdistance = batch_trace(temp, m, bs)\n",
        "\treturn -distance\n",
        "\n",
        "def IPOT_distance_torch_batch_uniform_T(C, bs, n, m, iteration=50):\n",
        "\tC = C.float().cuda()\n",
        "\tT = IPOT_torch_batch_uniform(C, bs, n, m, iteration=iteration)\n",
        "\t# temp = torch.bmm(torch.transpose(C,1,2), T)\n",
        "\t# distance = batch_trace(temp, m, bs)\n",
        "\treturn T\n",
        "\n",
        "\n",
        "def IPOT_torch_batch_uniform(C, bs, n, m, beta=0.5, iteration=50):\n",
        "\t# C is the distance matrix\n",
        "\t# c: bs by n by m\n",
        "\tsigma = torch.ones(bs, int(m), 1).cuda()/float(m)\n",
        "\tT = torch.ones(bs, n, m).cuda()\n",
        "\tA = torch.exp(-C/beta).float().cuda()\n",
        "\tfor t in range(iteration):\n",
        "\t\tQ = A * T # bs * n * m\n",
        "\t\tfor k in range(1):\n",
        "\t\t\tdelta = 1 / (n * torch.bmm(Q, sigma))\n",
        "\t\t\ta = torch.bmm(torch.transpose(Q,1,2), delta)\n",
        "\t\t\tsigma = 1 / (float(m) * a)\n",
        "\t\tT = delta * Q * sigma.transpose(2,1)\n",
        "\n",
        "\treturn T#.detach()\n",
        "\n",
        "\n",
        "def GW_distance(X, Y, p, q, lamda=0.5, iteration=5, OT_iteration=20):\n",
        "\t'''\n",
        "\t:param X, Y: Source and target embeddings , batchsize by embed_dim by n\n",
        "\t:param p, q: probability vectors\n",
        "\t:param lamda: regularization\n",
        "\t:return: GW distance\n",
        "\t'''\n",
        "\tCs = cos_batch_torch(X, X).float().cuda()\n",
        "\tCt = cos_batch_torch(Y, Y).float().cuda()\n",
        "\t# pdb.set_trace()\n",
        "\tbs = Cs.size(0)\n",
        "\tm = Ct.size(2)\n",
        "\tn = Cs.size(2)\n",
        "\tT, Cst = GW_torch_batch(Cs, Ct, bs, n, m, p, q, beta=lamda, iteration=iteration, OT_iteration=OT_iteration)\n",
        "\ttemp = torch.bmm(torch.transpose(Cst,1,2), T)\n",
        "\tdistance = batch_trace(temp, m, bs)\n",
        "\treturn distance\n",
        "\n",
        "def GW_torch_batch(Cs, Ct, bs, n, m, p, q, beta=0.5, iteration=5, OT_iteration=20):\n",
        "\tone_m = torch.ones(bs, m, 1).float().cuda()\n",
        "\tone_n = torch.ones(bs, n, 1).float().cuda()\n",
        "\n",
        "\tCst = torch.bmm(torch.bmm(Cs**2, p), torch.transpose(one_m, 1, 2)) + \\\n",
        "\t      torch.bmm(one_n, torch.bmm(torch.transpose(q,1,2), torch.transpose(Ct**2, 1, 2))) # bs by n by m\n",
        "\tgamma = torch.bmm(p, q.transpose(2,1)) # outer product, init\n",
        "\t# gamma = torch.einsum('bi,bj->bij', (torch.squeeze(p), torch.squeeze(q))) # outer product, initialization\n",
        "\tfor i in range(iteration):\n",
        "\t\tC_gamma = Cst - 2 * torch.bmm(torch.bmm(Cs, gamma), torch.transpose(Ct, 1, 2))\n",
        "\t\t# # Sinkhorn iteration\n",
        "\t\t# b = torch.ones(bs, m, 1).cuda()\n",
        "\t\t# K = torch.exp(-C_gamma/beta)\n",
        "\t\t# for i in range(50):cd\n",
        "\t\t# \ta = p/(torch.bmm(K, b))\n",
        "\t\t# \tb = q/torch.bmm(K.transpose(1,2), a)\n",
        "\t\t# gamma = a * K * b\n",
        "\t\tgamma = IPOT_torch_batch_uniform(C_gamma, bs, n, m, beta=beta, iteration=OT_iteration)\n",
        "\tCgamma = Cst - 2 * torch.bmm(torch.bmm(Cs, gamma), torch.transpose(Ct, 1, 2))\n",
        "\treturn gamma.detach(), Cgamma\n",
        "\n",
        "# def GW_torch_batch(Cs, Ct, bs, n, m, beta=0.5, iteration=5, OT_iteration=20):\n",
        "# \tone_m = torch.ones(bs, m, 1).float().cuda()\n",
        "# \tone_n = torch.ones(bs, n, 1).float().cuda()\n",
        "# \tp = (torch.ones(bs, m, 1)/m).cuda()\n",
        "# \tq = (torch.ones(bs, n, 1)/n).cuda()\n",
        "\n",
        "# \tCst = torch.bmm(torch.bmm(Cs**2, p), torch.transpose(one_m, 1, 2)) + \\\n",
        "# \t      torch.bmm(one_n, torch.bmm(torch.transpose(q,1,2), torch.transpose(Ct**2, 1, 2))) # bs by n by m\n",
        "# \tgamma = torch.bmm(p, q.transpose(2,1)) # outer product, init\n",
        "# \t# gamma = torch.einsum('bi,bj->bij', (torch.squeeze(p), torch.squeeze(q))) # outer product, initialization\n",
        "# \tfor i in range(iteration):\n",
        "# \t\tC_gamma = Cst - 2 * torch.bmm(torch.bmm(Cs, gamma), torch.transpose(Ct, 1, 2))\n",
        "# \t\tgamma = IPOT_torch_batch_uniform(C_gamma, bs, n, m, beta=beta, iteration=OT_iteration)\n",
        "# \tCgamma = Cst - 2 * torch.bmm(torch.bmm(Cs, gamma), torch.transpose(Ct, 1, 2))\n",
        "# \treturn gamma.detach(), Cgamma\n",
        "\n",
        "def GW_distance_uniform(X, Y, lamda=1e-1, iteration=5, OT_iteration=20):\n",
        "\tm = X.size(2)\n",
        "\tn = Y.size(2)\n",
        "\tbs = X.size(0)\n",
        "\tp = (torch.ones(bs, m, 1)/m).cuda()\n",
        "\tq = (torch.ones(bs, n, 1)/n).cuda()\n",
        "\treturn GW_distance(X, Y, p, q, lamda=lamda, iteration=iteration, OT_iteration=OT_iteration)\n",
        "\n",
        "\n",
        "def batch_diag(a_emb, n, bs):\n",
        "\ta = torch.eye(n).cuda().unsqueeze(0).repeat(bs, 1, 1) # bs * n * n\n",
        "\tb = (a_emb.unsqueeze(1).repeat(1,n,1))# bs * n * n\n",
        "\treturn a*b\n",
        "\t# diagonal bs by n by n\n",
        "\n",
        "def batch_trace(input_matrix, n, bs):\n",
        "\ta = torch.eye(n).cuda().unsqueeze(0).repeat(bs, 1, 1)\n",
        "\tb = a * input_matrix\n",
        "\treturn torch.sum(torch.sum(b,-1),-1).unsqueeze(1)"
      ],
      "metadata": {
        "id": "KeINbt1sxnFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GOTLoss(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.args = kwargs\n",
        "        self.got_lambda = self.args[\"got_lambda\"]\n",
        "\n",
        "    def GOT(self, v_, q_):\n",
        "        cos_distance = cost_matrix_batch_torch(v_.transpose(2, 1), q_.transpose(2, 1))\n",
        "        cos_distance = cos_distance.transpose(1,2)\n",
        "        beta = 0.1\n",
        "        min_score = cos_distance.min()\n",
        "        max_score = cos_distance.max()\n",
        "        threshold = min_score + beta * (max_score - min_score)\n",
        "        cos_dist = torch.nn.functional.relu(cos_distance - threshold)\n",
        "\n",
        "        wd = - IPOT_distance_torch_batch_uniform(cos_dist, v_.size(0), v_.size(1), q_.size(1), 30)\n",
        "        gwd = GW_distance_uniform(v_.transpose(2,1), q_.transpose(2,1))\n",
        "        twd = self.got_lambda * torch.mean(gwd) + self.got_lambda * torch.mean(wd) # Temporarily commented: #self.args.got_lambda *\n",
        "\n",
        "        return twd\n",
        "\n",
        "    def forward(self, v_, q_):\n",
        "        return self.GOT(v_, q_)\n"
      ],
      "metadata": {
        "id": "UtxZKSZbS3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "got = GOTLoss(got_lambda=0.1).GOT(image_embeddings, text_embeddings)"
      ],
      "metadata": {
        "id": "ikfnInMD3Gfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dbc0473-1d30-47f1-97fc-d6223f968db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'got_lambda': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "got"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHjZRfE4SZZW",
        "outputId": "80d9b3d0-3a91-429b-a989-cbb65f3a0778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1199, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}