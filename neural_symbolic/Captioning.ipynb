{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7076f3af-bda5-4b9d-9e1a-bee349d86b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a0d9d-1951-4ddd-bcaa-37fcb919aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate bitsandbytes\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "model_name=\"Salesforce/blip2-flan-t5-xl\"\n",
    "# model_name=\"Salesforce/blip2-opt-6.7b-coco\"\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbe8c5-3c5f-48b1-b565-3bcbed3e9f55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36f4846-5a3d-4fff-9f98-1caab7fdcf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing with a woman on the beach\n"
     ]
    }
   ],
   "source": [
    "# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# question = \"give a caption describing the action of the dog\"\n",
    "\n",
    "# # Image captioning (without providing a text prompt):\n",
    "# inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "# out = model.generate(**inputs)\n",
    "# print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fe8553-cb44-4eee-8069-f10a3d986eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman playing with dog on beach\n"
     ]
    }
   ],
   "source": [
    "# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# # Image captioning (without providing a text prompt):\n",
    "# inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "# out = model.generate(**inputs)\n",
    "# print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155060e6-d4c6-4683-82ae-6f4c0fc34f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half Precision\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# question = \"how many dogs are in the picture?\"\n",
    "# inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "# Full Precision\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", device_map=\"auto\")\n",
    "# question = \"how many dogs are in the picture?\"\n",
    "# inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addf3aa-809f-4738-9b2b-86dce6baf043",
   "metadata": {},
   "source": [
    "### Winoground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a145ea-6794-425f-9ff6-0238a9709fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "load_dotenv()  # This loads the environment variables from .env\n",
    "\n",
    "api_key = os.getenv('HGF_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544e6042-4fec-44f3-bf04-f30c770bd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "winoground = load_dataset('facebook/winoground', token=api_key)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b882a2-41b5-4c9d-88d7-50cf6b0b4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE=5\n",
    "NUM_BEAMS=5\n",
    "DO_SAMPLE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562a5bb7-3a85-402f-91e1-2f220aa806e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bottle is in water\n",
      "[[102, 7304, 16, 11, 514]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/efs/fs1/mmml/miniconda3/envs/cenv_x86/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a ina bottle is inaaaa bottle isa,']\n"
     ]
    }
   ],
   "source": [
    "example = winoground[7]\n",
    "# winoground dataset images are already in RGB mode\n",
    "raw_image_0 = example['image_0'] \n",
    "# # Image captioning (without providing a text prompt):\n",
    "inputs = processor(raw_image_0, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "# question = ' '.join(example['caption_1'].split(' ')[:2])\n",
    "# question = 'Question: is this the paired caption for this image: ' + example['caption_1']+ ' A:'\n",
    "# print(question)\n",
    "force_words = example['caption_0']\n",
    "print(force_words)\n",
    "force_words_ids = processor.tokenizer(force_words, add_special_tokens=False,return_tensors='pt').input_ids\n",
    "print(force_words_ids.tolist())\n",
    "# Image captioning prompted:\n",
    "# inputs = processor(raw_image_0, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "\n",
    "# out = model.generate(**inputs,do_sample=True,num_beams=5,num_return_sequences=SAMPLE_SIZE,\n",
    "# temperature=1)\n",
    "\n",
    "out=model.generate(\n",
    "    **inputs,\n",
    "    force_words_ids=force_words_ids.tolist(),\n",
    "    prefix_allowed_tokens_fn=lambda x,y:force_words_ids.tolist(),\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "print(processor.batch_decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a505c408-523c-48a1-8f97-fef56dc5c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man in a cowboy hat kissing a little girl on the cheek\n",
      "\n",
      "a man in cowboy hat kissing a little girl on the cheek\n",
      "\n",
      "a man in cowboy hat kissing a little girl on the cheek\n",
      "\n",
      "a man in a cowboy hat kissing a young girl on the cheek\n",
      "\n",
      "a man in cowboy hat kissing a young girl on the cheek\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for o in out:\n",
    "    print(processor.decode(o, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a060a3-8c61-4cc7-a39a-ec57049dcaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_on_data(winoground,f_stream):\n",
    "    for example in tqdm(winoground):\n",
    "        d = {}\n",
    "        d['id'] = example['id']\n",
    "    \n",
    "        for i in range(2):     \n",
    "            raw_image = example[f'image_{i}'] \n",
    "            inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "            out = model.generate(**inputs,do_sample=DO_SAMPLE,\n",
    "                                   num_beams=NUM_BEAMS,num_return_sequences=SAMPLE_SIZE)\n",
    "            d[f'caption_{i}'] = [processor.decode(o, skip_special_tokens=True) for o in out]\n",
    "    \n",
    "        print(json.dumps(d),file=f_stream,flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb199b85-2549-405d-a63c-5295891ff97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [46:51<00:00,  7.03s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(f'wino_{model_name.split(\"/\")[-1]}_caps.jsonl','w') as f:\n",
    "    generate_on_data(winoground,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2840866-4db0-4dd8-9047-f9d6807b8519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amr",
   "language": "python",
   "name": "cenv_x86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
